"""Kubernetes client factory.

Provides configured Kubernetes API clients for pod and job management.
Supports both in-cluster and out-of-cluster (kubeconfig) authentication.
"""

import os
from functools import lru_cache
from typing import Optional, Tuple

import structlog
from kubernetes import client, config
from kubernetes.client import (
    ApiException,
    BatchV1Api,
    CoreV1Api,
)

logger = structlog.get_logger(__name__)

# Global client instances
_core_api: CoreV1Api | None = None
_batch_api: BatchV1Api | None = None
_initialized: bool = False
_init_error: str | None = None


def _load_config() -> bool:
    """Load Kubernetes configuration.

    Tries in-cluster config first, falls back to kubeconfig.

    Returns:
        True if configuration was loaded successfully.
    """
    global _init_error

    # Try in-cluster config first (when running in a pod)
    try:
        config.load_incluster_config()
        logger.info("Loaded in-cluster Kubernetes configuration")
        return True
    except config.ConfigException:
        pass

    # Try kubeconfig file
    kubeconfig_path = os.getenv("KUBECONFIG", os.path.expanduser("~/.kube/config"))
    try:
        config.load_kube_config(config_file=kubeconfig_path)
        logger.info("Loaded kubeconfig", path=kubeconfig_path)
        return True
    except Exception as e:
        _init_error = f"Failed to load Kubernetes config: {e}"
        logger.error(_init_error)
        return False


def initialize_client() -> bool:
    """Initialize the Kubernetes client.

    Returns:
        True if initialization was successful.
    """
    global _core_api, _batch_api, _initialized, _init_error

    if _initialized:
        return _core_api is not None

    if not _load_config():
        _initialized = True
        return False

    try:
        _core_api = CoreV1Api()
        _batch_api = BatchV1Api()
        _initialized = True

        # Test the connection
        _core_api.get_api_resources()
        logger.info("Kubernetes client initialized successfully")
        return True

    except ApiException as e:
        _init_error = f"Kubernetes API error: {e.reason}"
        logger.error(_init_error)
        _initialized = True
        return False
    except Exception as e:
        _init_error = f"Failed to initialize Kubernetes client: {e}"
        logger.error(_init_error)
        _initialized = True
        return False


def get_kubernetes_client() -> tuple[CoreV1Api | None, BatchV1Api | None]:
    """Get the Kubernetes API clients.

    Returns:
        Tuple of (CoreV1Api, BatchV1Api) or (None, None) if not available.
    """
    if not _initialized:
        initialize_client()

    return _core_api, _batch_api


def get_core_api() -> CoreV1Api | None:
    """Get the Core V1 API client for pod operations."""
    core, _ = get_kubernetes_client()
    return core


def get_batch_api() -> BatchV1Api | None:
    """Get the Batch V1 API client for job operations."""
    _, batch = get_kubernetes_client()
    return batch


def is_available() -> bool:
    """Check if Kubernetes client is available."""
    if not _initialized:
        initialize_client()
    return _core_api is not None


def get_initialization_error() -> str | None:
    """Get the initialization error message if any."""
    return _init_error


def get_current_namespace() -> str:
    """Get the current namespace.

    When running in-cluster, reads from the service account.
    Otherwise, uses the default namespace or NAMESPACE env var.
    """
    # Check environment variable first
    namespace = os.getenv("NAMESPACE", os.getenv("POD_NAMESPACE"))
    if namespace:
        return namespace

    # Try to read from service account (in-cluster)
    try:
        with open("/var/run/secrets/kubernetes.io/serviceaccount/namespace") as f:
            return f.read().strip()
    except (OSError, FileNotFoundError):
        pass

    # Default namespace
    return "default"


class KubernetesClientContext:
    """Context manager for Kubernetes operations.

    Provides convenient access to API clients with error handling.
    """

    def __init__(self):
        self.core_api: CoreV1Api | None = None
        self.batch_api: BatchV1Api | None = None
        self.namespace: str = get_current_namespace()

    def __enter__(self):
        self.core_api, self.batch_api = get_kubernetes_client()
        if not self.core_api:
            raise RuntimeError(f"Kubernetes client not available: {get_initialization_error()}")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # No cleanup needed for now
        pass


def create_pod_manifest(
    name: str,
    namespace: str,
    main_image: str,
    sidecar_image: str,
    language: str,
    labels: dict,
    annotations: dict = None,
    cpu_limit: str = "1",
    memory_limit: str = "512Mi",
    cpu_request: str = "100m",
    memory_request: str = "128Mi",
    run_as_user: int = 65532,
    sidecar_port: int = 8080,
    image_pull_policy: str = "Always",
    sidecar_cpu_limit: str = "500m",
    sidecar_memory_limit: str = "512Mi",
    sidecar_cpu_request: str = "100m",
    sidecar_memory_request: str = "256Mi",
    seccomp_profile_type: str = "RuntimeDefault",
    network_isolated: bool = False,
    execution_mode: str = "agent",
    executor_port: int = 9090,
    gke_sandbox_enabled: bool = False,
    runtime_class_name: str = "gvisor",
    sandbox_node_selector: dict[str, str] | None = None,
    custom_tolerations: list[dict[str, str]] | None = None,
    image_pull_secrets: list[str] | None = None,
) -> client.V1Pod:
    """Create a Pod manifest for code execution.

    Supports two execution modes:

    - agent (default): An executor agent runs in the main container, providing
      HTTP-based code execution. No nsenter, no capabilities, no privilege
      escalation needed. Compatible with GKE Sandbox (gVisor) and restricted
      Pod Security Standards.

    - nsenter (legacy): The sidecar uses nsenter to enter the main container's
      mount namespace. Requires SYS_PTRACE, SYS_ADMIN, SYS_CHROOT capabilities,
      shareProcessNamespace, and allowPrivilegeEscalation: true.

    Args:
        name: Pod name
        namespace: Kubernetes namespace
        main_image: Image for the main (language) container
        sidecar_image: Image for the sidecar container
        language: Programming language
        labels: Pod labels
        annotations: Pod annotations
        cpu_limit: CPU limit
        memory_limit: Memory limit
        cpu_request: CPU request
        memory_request: Memory request
        run_as_user: UID to run containers as
        sidecar_port: Port for sidecar HTTP API
        seccomp_profile_type: Seccomp profile type (RuntimeDefault or Unconfined)
        network_isolated: Whether network isolation is enabled
        execution_mode: Execution mode - "agent" (default) or "nsenter"
        executor_port: Port for the executor HTTP server inside the main container
        gke_sandbox_enabled: Enable GKE Sandbox (gVisor) for additional kernel isolation
        runtime_class_name: Runtime class name for sandboxed pods (default: gvisor)
        sandbox_node_selector: Node selector for sandbox-enabled nodes
        custom_tolerations: Additional tolerations for custom node pool taints
        image_pull_secrets: List of secret names for pulling images from private registries

    Returns:
        V1Pod manifest ready for creation.
    """
    use_agent = execution_mode == "agent"

    # Warn if GKE Sandbox is enabled with nsenter mode (incompatible with gVisor)
    if gke_sandbox_enabled and not use_agent:
        logger.warning(
            "GKE Sandbox (gVisor) is enabled but execution mode is 'nsenter'. "
            "nsenter requires capabilities incompatible with gVisor. "
            "Consider switching to 'agent' execution mode.",
            execution_mode=execution_mode,
            gke_sandbox_enabled=gke_sandbox_enabled,
        )

    # Shared volume for code and data
    shared_volume = client.V1Volume(
        name="shared-data",
        empty_dir=client.V1EmptyDirVolumeSource(
            medium="",
            size_limit="1Gi",
        ),
    )

    shared_mount = client.V1VolumeMount(
        name="shared-data",
        mount_path="/mnt/data",
    )

    # Security context for main container - minimal privileges in both modes
    main_security_context = client.V1SecurityContext(
        run_as_user=run_as_user,
        run_as_group=run_as_user,
        run_as_non_root=True,
        allow_privilege_escalation=False,
        capabilities=client.V1Capabilities(drop=["ALL"]),
    )

    if use_agent:
        # Agent mode: sidecar also has minimal privileges (no nsenter needed)
        sidecar_security_context = client.V1SecurityContext(
            run_as_user=run_as_user,
            run_as_group=run_as_user,
            run_as_non_root=True,
            allow_privilege_escalation=False,
            capabilities=client.V1Capabilities(drop=["ALL"]),
        )
    else:
        # nsenter mode: sidecar needs elevated privileges for nsenter
        #
        # The sidecar uses nsenter to execute code in the main container's mount namespace.
        # nsenter requires these capabilities:
        # - SYS_PTRACE: access /proc/<pid>/ns/ of other processes
        # - SYS_ADMIN: call setns() to enter namespaces
        # - SYS_CHROOT: required for mount namespace operations
        #
        # File capabilities (setcap on nsenter) require allowPrivilegeEscalation: true.
        sidecar_security_context = client.V1SecurityContext(
            run_as_user=run_as_user,
            run_as_group=run_as_user,
            run_as_non_root=True,
            allow_privilege_escalation=True,
            capabilities=client.V1Capabilities(
                add=["SYS_PTRACE", "SYS_ADMIN", "SYS_CHROOT"],
                drop=["ALL"],
            ),
        )

    # Resource requirements
    resources = client.V1ResourceRequirements(
        limits={"cpu": cpu_limit, "memory": memory_limit},
        requests={"cpu": cpu_request, "memory": memory_request},
    )

    # Main container (language runtime)
    main_container = client.V1Container(
        name="main",
        image=main_image,
        image_pull_policy=image_pull_policy,
        volume_mounts=[shared_mount],
        security_context=main_security_context,
        resources=resources,
        env=[
            client.V1EnvVar(name="PYTHONUNBUFFERED", value="1"),
            client.V1EnvVar(name="HOME", value="/mnt/data"),
        ],
    )

    # In agent mode, override CMD to run the executor agent from the shared volume
    # (copied there by the init container)
    if use_agent:
        main_container.args = ["/mnt/data/.executor-agent"]

    # Sidecar environment variables
    sidecar_env = [
        client.V1EnvVar(name="LANGUAGE", value=language),
        client.V1EnvVar(name="WORKING_DIR", value="/mnt/data"),
        client.V1EnvVar(name="SIDECAR_PORT", value=str(sidecar_port)),
        client.V1EnvVar(name="NETWORK_ISOLATED", value=str(network_isolated).lower()),
        client.V1EnvVar(name="EXECUTION_MODE", value=execution_mode),
        client.V1EnvVar(name="EXECUTOR_PORT", value=str(executor_port)),
    ]

    # Sidecar container (HTTP API)
    sidecar_container = client.V1Container(
        name="sidecar",
        image=sidecar_image,
        image_pull_policy=image_pull_policy,
        ports=[client.V1ContainerPort(container_port=sidecar_port, name="http")],
        volume_mounts=[shared_mount],
        security_context=sidecar_security_context,
        resources=client.V1ResourceRequirements(
            # In nsenter mode: user code runs in the sidecar's cgroup via nsenter
            # In agent mode: sidecar only proxies requests, user code runs in main container
            limits={"cpu": sidecar_cpu_limit, "memory": sidecar_memory_limit},
            requests={"cpu": sidecar_cpu_request, "memory": sidecar_memory_request},
        ),
        env=sidecar_env,
        readiness_probe=client.V1Probe(
            http_get=client.V1HTTPGetAction(path="/ready", port=sidecar_port),
            initial_delay_seconds=5,
            period_seconds=3,
            timeout_seconds=5,
            failure_threshold=5,
        ),
        liveness_probe=client.V1Probe(
            http_get=client.V1HTTPGetAction(path="/health", port=sidecar_port),
            initial_delay_seconds=5,
            period_seconds=10,
            timeout_seconds=5,
            failure_threshold=3,
        ),
    )

    # Init containers (agent mode only)
    # Copy the executor agent binary from the sidecar image to the shared volume
    init_containers = None
    if use_agent:
        init_containers = [
            client.V1Container(
                name="agent-init",
                image=sidecar_image,
                image_pull_policy=image_pull_policy,
                command=[
                    "python",
                    "-c",
                    "import shutil, os; shutil.copy2('/opt/executor-agent', '/mnt/data/.executor-agent'); os.chmod('/mnt/data/.executor-agent', 0o755)",
                ],
                volume_mounts=[shared_mount],
                security_context=client.V1SecurityContext(
                    run_as_user=run_as_user,
                    run_as_group=run_as_user,
                    run_as_non_root=True,
                    allow_privilege_escalation=False,
                    capabilities=client.V1Capabilities(drop=["ALL"]),
                ),
                resources=client.V1ResourceRequirements(
                    limits={"cpu": "100m", "memory": "64Mi"},
                    requests={"cpu": "50m", "memory": "32Mi"},
                ),
            )
        ]

    # GKE Sandbox configuration
    # When enabled, adds gVisor runtime, node selector, and tolerations
    runtime_class = runtime_class_name if gke_sandbox_enabled else None

    # Build node selector
    node_selector = {}
    if gke_sandbox_enabled:
        # GKE automatically adds this label to sandbox-enabled nodes
        node_selector["sandbox.gke.io/runtime"] = "gvisor"
    if sandbox_node_selector:
        node_selector.update(sandbox_node_selector)

    # Build tolerations list
    tolerations = []
    if gke_sandbox_enabled:
        # GKE Sandbox standard taint
        tolerations.append(
            client.V1Toleration(
                key="sandbox.gke.io/runtime",
                operator="Equal",
                value="gvisor",
                effect="NoSchedule",
            )
        )
    if custom_tolerations:
        # Add custom node pool taints (e.g., pool=sandbox)
        for tol in custom_tolerations:
            tol_key = tol.get("key")
            if not tol_key:
                logger.warning("Skipping custom toleration with missing 'key' field", toleration=tol)
                continue
            tolerations.append(
                client.V1Toleration(
                    key=tol_key,
                    operator=tol.get("operator", "Equal"),
                    value=tol.get("value"),
                    effect=tol.get("effect", "NoSchedule"),
                )
            )

    # Build image pull secrets list
    pull_secrets = None
    if image_pull_secrets:
        pull_secrets = [client.V1LocalObjectReference(name=secret_name) for secret_name in image_pull_secrets]

    # Pod spec
    pod_spec = client.V1PodSpec(
        init_containers=init_containers,
        containers=[main_container, sidecar_container],
        volumes=[shared_volume],
        restart_policy="Never",
        termination_grace_period_seconds=10,
        # Share process namespace only needed for nsenter mode
        share_process_namespace=not use_agent,
        runtime_class_name=runtime_class,
        node_selector=node_selector if node_selector else None,
        tolerations=tolerations if tolerations else None,
        image_pull_secrets=pull_secrets,
        security_context=client.V1PodSecurityContext(
            fs_group=run_as_user,
            seccomp_profile=client.V1SeccompProfile(type=seccomp_profile_type),
        ),
    )

    # Pod metadata
    # Add GKE Sandbox annotation if enabled
    pod_annotations = dict(annotations) if annotations else {}
    if gke_sandbox_enabled:
        # GKE Sandbox annotation for gVisor runtime
        pod_annotations["sandbox.gke.io/runtime"] = "gvisor"

    metadata = client.V1ObjectMeta(
        name=name,
        namespace=namespace,
        labels=labels,
        annotations=pod_annotations,
    )

    return client.V1Pod(
        api_version="v1",
        kind="Pod",
        metadata=metadata,
        spec=pod_spec,
    )


def create_job_manifest(
    name: str,
    namespace: str,
    main_image: str,
    sidecar_image: str,
    language: str,
    labels: dict,
    ttl_seconds_after_finished: int = 60,
    active_deadline_seconds: int = 300,
    **kwargs,
) -> client.V1Job:
    """Create a Job manifest for code execution.

    Jobs are used for cold-path languages where we don't maintain
    a warm pod pool.

    Args:
        name: Job name
        namespace: Kubernetes namespace
        main_image: Image for the main container
        sidecar_image: Image for the sidecar container
        language: Programming language
        labels: Job labels
        ttl_seconds_after_finished: TTL for completed jobs
        active_deadline_seconds: Maximum execution time

    Returns:
        V1Job manifest ready for creation.
    """
    # Create pod template using the same logic
    pod = create_pod_manifest(
        name=f"{name}-pod",
        namespace=namespace,
        main_image=main_image,
        sidecar_image=sidecar_image,
        language=language,
        labels=labels,
        **kwargs,
    )

    # Job spec
    job_spec = client.V1JobSpec(
        template=client.V1PodTemplateSpec(
            metadata=pod.metadata,
            spec=pod.spec,
        ),
        backoff_limit=0,  # Don't retry failed jobs
        ttl_seconds_after_finished=ttl_seconds_after_finished,
        active_deadline_seconds=active_deadline_seconds,
    )

    # Job metadata
    metadata = client.V1ObjectMeta(
        name=name,
        namespace=namespace,
        labels=labels,
    )

    return client.V1Job(
        api_version="batch/v1",
        kind="Job",
        metadata=metadata,
        spec=job_spec,
    )
